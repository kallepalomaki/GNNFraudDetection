{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fda4b0-72ce-4e28-9c2a-b9d9248a6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample Python script.\n",
    "import pandas as pd\n",
    "import torch\n",
    "device = torch.device('cpu')\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18b6091-298f-4e96-933a-64499e87ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         step      type     amount     nameOrig  oldbalanceOrg  \\\n",
      "5615431   395  TRANSFER  308362.85   C140079217            0.0   \n",
      "4012845   299  CASH_OUT  255018.29   C665653517         2010.0   \n",
      "4178020   304   PAYMENT   55154.11   C451947605            0.0   \n",
      "6207478   587   PAYMENT   25232.50   C461231178        80849.0   \n",
      "482370     19   PAYMENT    1819.55  C1654714032       238819.0   \n",
      "\n",
      "         newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  isFraud  \\\n",
      "5615431            0.00  C2051203950       817878.27      1126241.12        0   \n",
      "4012845            0.00   C463826132      4685364.02      4940382.31        0   \n",
      "4178020            0.00   M404968312            0.00            0.00        0   \n",
      "6207478        55616.50   M880297774            0.00            0.00        0   \n",
      "482370        236999.45   M451967605            0.00            0.00        0   \n",
      "\n",
      "         isFlaggedFraud  \n",
      "5615431               0  \n",
      "4012845               0  \n",
      "4178020               0  \n",
      "6207478               0  \n",
      "482370                0  \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/PaySim_kaggle.csv\")\n",
    "\n",
    "df=df.sample(n=5000000)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65c8bcb-5856-4d91-9eb1-8a090c0b2556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud count: 6544\n",
      "Non-fraud count: 49935\n",
      "Fraud ratio: 0.1159\n",
      "Non-fraud ratio: 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Set the proportion of non-fraud to remove (e.g., 50% of non-fraud instances)\n",
    "remove_fraction = 0.99\n",
    "\n",
    "# Separate the fraud and non-fraud instances\n",
    "fraud_df = df[df['isFraud'] == 1]\n",
    "non_fraud_df = df[df['isFraud'] == 0]\n",
    "\n",
    "# Randomly sample and remove 'remove_fraction' proportion of non-fraud instances\n",
    "non_fraud_to_remove = non_fraud_df.sample(frac=remove_fraction, random_state=42)\n",
    "\n",
    "# Drop the sampled non-fraud instances from the DataFrame\n",
    "df = df.drop(non_fraud_to_remove.index)\n",
    "\n",
    "# Verify the new balance\n",
    "label_counts = df['isFraud'].value_counts()\n",
    "fraud_ratio = label_counts[1] / len(df)\n",
    "non_fraud_ratio = label_counts[0] / len(df)\n",
    "\n",
    "print(f\"Fraud count: {label_counts[1]}\")\n",
    "print(f\"Non-fraud count: {label_counts[0]}\")\n",
    "print(f\"Fraud ratio: {fraud_ratio:.4f}\")\n",
    "print(f\"Non-fraud ratio: {non_fraud_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c68098-4397-40d0-b946-7a0230703f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from unique user names to numeric IDs (nodes)\n",
    "user_mapping = {user: idx for idx, user in enumerate(set(df['nameOrig']).union(set(df['nameDest'])))}\n",
    "\n",
    "# Create edges between nameOrig and nameDest\n",
    "src = df['nameOrig'].map(user_mapping).values\n",
    "dst = df['nameDest'].map(user_mapping).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfd81d5-b2e8-4941-bc11-f3bd4257dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 76455  99097 106274 ...  80441   8708  82720]\n"
     ]
    }
   ],
   "source": [
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5253dc18-6c5e-4523-ab68-d45b6984bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DGL graph from the source and destination nodes\n",
    "g = dgl.graph((src, dst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d8d10c-4b83-4564-a4d1-280fae10d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transaction amount as edge feature\n",
    "g.edata['amount'] = torch.tensor(df['amount'].values, dtype=torch.float32)\n",
    "\n",
    "# Optional: Add fraud information to edge features\n",
    "g.edata['isFraud'] = torch.tensor(df['isFraud'].values, dtype=torch.float32)\n",
    "\n",
    "# Initialize node features with zeros (this handles all nodes)\n",
    "num_nodes = g.num_nodes()\n",
    "balance_orig = torch.zeros(num_nodes, dtype=torch.float32)\n",
    "balance_dest = torch.zeros(num_nodes, dtype=torch.float32)\n",
    "\n",
    "for orig_user, balance in df[['nameOrig', 'oldbalanceOrg']].drop_duplicates().values:\n",
    "    balance_orig[user_mapping[orig_user]] = balance\n",
    "\n",
    "for dest_user, balance in df[['nameDest', 'oldbalanceDest']].drop_duplicates().values:\n",
    "    balance_dest[user_mapping[dest_user]] = balance\n",
    "\n",
    "node_features = torch.stack([balance_orig, balance_dest], dim=1)  # Changed to stack both features\n",
    "\n",
    "g.ndata['features'] = node_features\n",
    "\n",
    "node_labels = torch.zeros(num_nodes, dtype=torch.float32)\n",
    "\n",
    "# Map 'isFraud' values to corresponding nodes based on user mapping\n",
    "for user, fraud in df[['nameOrig', 'isFraud']].drop_duplicates().values:\n",
    "    node_labels[user_mapping[user]] = fraud\n",
    "\n",
    "\n",
    "# Map 'isFraud' to destination nodes (nameDest)\n",
    "for dest_user, fraud in df[['nameDest', 'isFraud']].drop_duplicates().values:\n",
    "    node_labels[user_mapping[dest_user]] = fraud\n",
    "\n",
    "# Store the fraud labels in g.ndata['isFraud']\n",
    "g.ndata['isFraud'] = node_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e37e5f8-5d2f-492e-8fcc-db40ce4f3e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DGLGraph.num_edges of Graph(num_nodes=110779, num_edges=56479,\n",
       "      ndata_schemes={'features': Scheme(shape=(2,), dtype=torch.float32), 'isFraud': Scheme(shape=(), dtype=torch.float32)}\n",
       "      edata_schemes={'amount': Scheme(shape=(), dtype=torch.float32), 'isFraud': Scheme(shape=(), dtype=torch.float32)})>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edata['isFraud'].shape\n",
    "#node_features.shape\n",
    "g.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788366e2-8e69-40f7-82d7-5bb92f9a1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print graph information\n",
    "#print(g)\n",
    "\n",
    "# Visualize the graph using NetworkX (convert DGL graph to NetworkX for visualization)\n",
    "#nx_graph = g.to_networkx()\n",
    "\n",
    "# Optional: Visualize using a layout for better readability\n",
    "#pos = nx.spring_layout(nx_graph)  # Use a layout for better visualization\n",
    "#plt.figure(figsize=(12, 12))\n",
    "#nx.draw(nx_graph, pos, node_size=50, node_color='skyblue', font_size=10, with_labels=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe561538-118f-4c17-b6cc-54460d44f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Features (Transaction Amounts):\n",
      "tensor([9.6495e+04, 8.8894e+03, 1.9429e+03,  ..., 1.4209e+04, 2.1160e+01,\n",
      "        1.2166e+05])\n"
     ]
    }
   ],
   "source": [
    "# Assuming that you have edge features like transaction amounts or fraud status\n",
    "edge_features = g.edata.get('amount', None)  # Assuming 'amount' is an edge feature\n",
    "if edge_features is not None:\n",
    "    print(\"Edge Features (Transaction Amounts):\")\n",
    "    print(edge_features)\n",
    "else:\n",
    "    print(\"No edge features found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecd2e705-81a6-4e96-a46a-7e8af83ea77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 80% of nodes for training\n",
    "num_train_nodes = int(0.8 * num_nodes)\n",
    "train_indices = torch.randperm(num_nodes)[:num_train_nodes]  \n",
    "test_indices = torch.tensor([i for i in range(num_nodes) if i not in train_indices])\n",
    "\n",
    "# Create train/test masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "# Assign to graph\n",
    "g.ndata['train_mask'] = train_mask\n",
    "g.ndata['test_mask'] = test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b521fd7-bdfd-47eb-a894-4f4175fd2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphSAGE model for fraud detection\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layer1 = dgl.nn.SAGEConv(in_feats, hidden_feats, 'mean')\n",
    "        self.layer2 = dgl.nn.SAGEConv(hidden_feats, out_feats, 'mean')\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        #self.fc = nn.Linear(out_feats * 2, 1)  # * Concatenate source and destination node embeddings *\n",
    "        self.fc = nn.Linear(out_feats, 1)  # Output a single value per node (fraud score)\n",
    "\n",
    "    def forward_old2(self, blocks, features):\n",
    "        # Apply first GraphSAGE layer and ReLU\n",
    "        x = self.layer1(blocks[0], features)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply second GraphSAGE layer to get node embeddings\n",
    "        x = self.layer2(blocks[1], x)\n",
    "\n",
    "        # Get source and destination nodes for each edge\n",
    "        src, dst = blocks[1].edges()  # Use the second block for destination nodes\n",
    "        src_local, dst_local = blocks[1].srcdata['_ID'], blocks[1].dstdata['_ID']\n",
    "\n",
    "        # Embeddings for source and destination nodes (local indices)\n",
    "        print(src_local[1:5])\n",
    "        print(x)\n",
    "        src_embeddings = x[src_local]  # Embeddings for source nodes\n",
    "        dst_embeddings = x[dst_local]  # Embeddings for destination nodes\n",
    "        \n",
    "        # Concatenate source and destination node embeddings to create edge features\n",
    "        edge_features = torch.cat([src_embeddings, dst_embeddings], dim=1)  # Concatenate along the feature dimension\n",
    "        \n",
    "        # Output a prediction for each edge (fraud score)\n",
    "        logits = self.fc(edge_features).squeeze()  # Output a single value per edge\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        # Apply first GraphSAGE layer and ReLU\n",
    "        x = self.layer1(g, features)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply second GraphSAGE layer to get node embeddings\n",
    "        x = self.layer2(g, x)\n",
    "        \n",
    "        # Output a prediction for each node\n",
    "        logits = self.fc(x).squeeze()  # Output a single value per node\n",
    "        return logits\n",
    "        \n",
    "\n",
    "    def forward_old(self, blocks, features):\n",
    "        # Apply first GraphSAGE layer and ReLU\n",
    "        x = self.layer1(blocs[0], features)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply second GraphSAGE layer to get node embeddings\n",
    "        x = self.layer2(blocs[1], x)\n",
    "\n",
    "        # Get source and destination nodes for each edge\n",
    "        src, dst = blocks[1].edges()  # Use the second block for destination nodes\n",
    "        src_local, dst_local = blocks[1].srcdata['_ID'], blocks[1].dstdata['_ID']\n",
    "        \n",
    "        # Map to global node IDs\n",
    "        src_global = blocks[1].srcdata['_ID'][src]\n",
    "        dst_global = blocks[1].dstdata['_ID'][dst]\n",
    "        src_embeddings = x[src_local]  # Embeddings for source nodes\n",
    "        dst_embeddings = x[dst_local]  # Embeddings for destination nodes\n",
    "        \n",
    "        # Concatenate source and destination node embeddings to create edge features\n",
    "        edge_features = torch.cat([src_embeddings, dst_embeddings], dim=1)  # Concatenate along the feature dimension\n",
    "        \n",
    "        # Output a prediction for each node\n",
    "        logits = self.fc(edge_features).squeeze()  # Output a single value per node\n",
    "        return logits\n",
    "        \n",
    "    def forward_edge(self, g, features):\n",
    "        # Apply first GraphSAGE layer and ReLU\n",
    "        x = self.layer1(g, features)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply second GraphSAGE layer to get embeddings\n",
    "        x = self.layer2(g, x)\n",
    "        \n",
    "        # Get source and destination nodes for each edge\n",
    "        src, dst = g.edges()  # Get indices of source and destination nodes\n",
    "        src_embeddings = x[src]  # Embeddings for source nodes\n",
    "        dst_embeddings = x[dst]  # Embeddings for destination nodes\n",
    "        \n",
    "        # Concatenate source and destination node embeddings to create edge features\n",
    "        edge_features = torch.cat([src_embeddings, dst_embeddings], dim=1)  # Concatenate along the feature dimension\n",
    "        logits = self.fc(edge_features).squeeze()  # * Output a single value per edge (fraud score) *\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf5f966-4635-469e-a2d6-2bf70d46c371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud count: 6544\n",
      "Non-fraud count: 49935\n",
      "Fraud ratio: 0.1159\n",
      "Non-fraud ratio: 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each label in the 'isFraud' column\n",
    "label_counts = df['isFraud'].value_counts()\n",
    "\n",
    "# Calculate the proportion of each class\n",
    "fraud_ratio = label_counts[1] / len(df)  # Assuming '1' represents fraud\n",
    "non_fraud_ratio = label_counts[0] / len(df)  # Assuming '0' represents non-fraud\n",
    "\n",
    "# Print the results\n",
    "print(f\"Fraud count: {label_counts[1]}\")\n",
    "print(f\"Non-fraud count: {label_counts[0]}\")\n",
    "print(f\"Fraud ratio: {fraud_ratio:.4f}\")\n",
    "print(f\"Non-fraud ratio: {non_fraud_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baf51cee-07ff-426e-8d97-8d8c956596a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import NeighborSampler, DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 512  \n",
    "\n",
    "# Define a NeighborSampler (sampling 10 neighbors per layer)\n",
    "sampler = NeighborSampler([10, 10])  # 2-layer GraphSAGE with 10 neighbors per node\n",
    "\n",
    "g.ndata['train_mask']=train_mask  # Boolean mask for training nodes\n",
    "train_nid = torch.nonzero(train_mask, as_tuple=True)[0]  # Get indices\n",
    "\n",
    "# Create a DataLoader for mini-batching\n",
    "train_dataloader = DataLoader(\n",
    "    g,                # Full DGL graph\n",
    "    train_nid,        # Training node IDs (subset of g.ndata['train_mask'])\n",
    "    sampler,          # Neighbor sampler\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820da612-3b77-4036-b63b-e4ddc59fc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compose_graph_from_blocks(blocks):\n",
    "    # Extract the nodes and edges from the blocks\n",
    "    block1, block2 = blocks\n",
    "\n",
    "    # Get source and destination nodes for both blocks\n",
    "    src_nodes_block1 = block1.srcdata['_ID']\n",
    "    dst_nodes_block1 = block1.dstdata['_ID']\n",
    "    \n",
    "    src_nodes_block2 = block2.srcdata['_ID']\n",
    "    dst_nodes_block2 = block2.dstdata['_ID']\n",
    "\n",
    "    # Get the edges (src, dst) for both blocks\n",
    "    src_edges_block1, dst_edges_block1 = block1.edges()\n",
    "    src_edges_block2, dst_edges_block2 = block2.edges()\n",
    "\n",
    "    # Create a new graph combining the nodes and edges from both blocks\n",
    "    # Add all the edges and nodes from both blocks\n",
    "    edges_src_all = torch.cat([src_edges_block1, src_edges_block2])\n",
    "    edges_dst_all = torch.cat([dst_edges_block1, dst_edges_block2])\n",
    "\n",
    "    # Create a new graph from these nodes and edges\n",
    "    g = dgl.graph((edges_src_all, edges_dst_all))\n",
    "\n",
    "    # Ensure that the node IDs are consistent across the blocks\n",
    "    # Create a mapping from block-local node IDs to global node IDs\n",
    "    node_id_mapping = {**dict(zip(src_nodes.tolist(), range(len(src_nodes)))), \n",
    "                       **dict(zip(dst_nodes.tolist(), range(len(dst_nodes))))}\n",
    "\n",
    "    # Concatenate node features from both blocks, ensuring correct feature assignment\n",
    "    src_features = block1.srcdata['features']\n",
    "    dst_features = block2.dstdata['features']\n",
    "\n",
    "    # The final number of features should match the total number of nodes\n",
    "    all_features = torch.cat([src_features, dst_features], dim=0)\n",
    "        # Print out some debugging information to understand the shapes\n",
    "    print(f\"Shape of src_features: {src_features.shape}\")\n",
    "    print(f\"Shape of dst_features: {dst_features.shape}\")\n",
    "    print(f\"Shape of all_features: {all_features.shape}\")\n",
    "    print(f\"Number of nodes in composed graph: {g.num_nodes()}\")\n",
    "\n",
    "    # Ensure the number of features matches the number of nodes in the graph\n",
    "    assert len(all_features) == g.num_nodes(), \"Mismatch between features and nodes.\"\n",
    "\n",
    "    # Set the node features for the composed graph\n",
    "    g.ndata['features'] = all_features\n",
    "\n",
    "    return g\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f542233-e155-4918-8816-b1e3e427d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/6ls0dvzn3v5_wbhst3zymxz40000gn/T/ipykernel_36988/725254986.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_nodes_batch = torch.tensor(input_nodes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Loss: 53319.703125\n",
      "Epoch 4/50, Loss: 18826.716796875\n",
      "Epoch 6/50, Loss: 5770.92041015625\n",
      "Epoch 8/50, Loss: 375.16619873046875\n",
      "Epoch 10/50, Loss: 15.654949188232422\n",
      "Epoch 12/50, Loss: 3.575941324234009\n",
      "Epoch 14/50, Loss: 9.680578231811523\n",
      "Epoch 16/50, Loss: 14.071416854858398\n",
      "Epoch 18/50, Loss: 18.81200408935547\n",
      "Epoch 20/50, Loss: 14.072147369384766\n",
      "Epoch 22/50, Loss: 4.734349727630615\n",
      "Epoch 24/50, Loss: 1.4067397117614746\n",
      "Epoch 26/50, Loss: 4.717514991760254\n",
      "Epoch 28/50, Loss: 4.274027347564697\n",
      "Epoch 30/50, Loss: 2.1701550483703613\n",
      "Epoch 32/50, Loss: 9.987435340881348\n",
      "Epoch 34/50, Loss: 6.464606285095215\n",
      "Epoch 36/50, Loss: 6.07663631439209\n",
      "Epoch 38/50, Loss: 5.622628211975098\n",
      "Epoch 40/50, Loss: 5.088200569152832\n",
      "Epoch 42/50, Loss: 1.4591577053070068\n",
      "Epoch 44/50, Loss: 9.314507484436035\n",
      "Epoch 46/50, Loss: 8.872360229492188\n",
      "Epoch 48/50, Loss: 6.083202362060547\n",
      "Epoch 50/50, Loss: 11.18979549407959\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "in_feats = 2  # balanceOrig and balanceDest features\n",
    "hidden_feats = 64\n",
    "out_feats = 1  # Fraud (binary classification)\n",
    "\n",
    "pos_weight = torch.tensor([non_fraud_ratio/fraud_ratio],dtype=torch.float)\n",
    "#pos_weight = torch.tensor([1.0],dtype=torch.float)\n",
    "\n",
    "\n",
    "model = GraphSAGE(in_feats, hidden_feats, out_feats)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for step, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        #print(len(blocks))\n",
    "        #composed_graph = compose_graph_from_blocks(blocks)\n",
    "        input_features=g.ndata['features'][input_nodes]\n",
    "        input_nodes_batch = torch.tensor(input_nodes)\n",
    "        subgraph = dgl.node_subgraph(g, input_nodes_batch)\n",
    "        logits = model(subgraph, input_features)\n",
    "        #print(logits.shape)\n",
    "    \n",
    "        # Get target labels for the fraud detection task\n",
    "        labels = g.ndata['isFraud'][input_nodes]\n",
    "        breakpoint()\n",
    "        # Compute loss (use train_mask to filter out test edges)\n",
    "        #print(\"labels\",labels[train_mask].view(-1,1).shape)\n",
    "        #print(\"logits\", logits[train_mask].view(-1,1).shape)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Print loss every few epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c15695-ca6e-49ef-81e7-b66b7618a1d7",
   "metadata": {},
   "source": [
    "Epoch 2/20, Loss: 50026236.0\n",
    "Epoch 4/20, Loss: 21910472.0\n",
    "Epoch 6/20, Loss: 12671362.0\n",
    "Epoch 8/20, Loss: 12492275.0\n",
    "Epoch 10/20, Loss: 13373789.0\n",
    "Epoch 12/20, Loss: 14267830.0\n",
    "Epoch 14/20, Loss: 14137046.0\n",
    "Epoch 16/20, Loss: 12949893.0\n",
    "Epoch 18/20, Loss: 11810090.0\n",
    "Epoch 20/20, Loss: 10091464.0\n",
    "0.1\n",
    "Accuracy: 0.7332\n",
    "Recall: 0.0086\n",
    "F1 Score: 0.0073\n",
    "0.01\n",
    "Accuracy: 0.7535\n",
    "Recall: 0.0039\n",
    "F1 Score: 0.0036\n",
    "0.2\n",
    "Accuracy: 0.7378\n",
    "Recall: 0.0102\n",
    "F1 Score: 0.0087\n",
    "0.4\n",
    "Accuracy: 0.7405\n",
    "Recall: 0.0079\n",
    "F1 Score: 0.0068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d073edc-243e-4b6e-9d3e-9ac7d47ce144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'isFraud', 'train_mask', 'test_mask'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e85df0-9f6b-456d-82b1-b8e41577a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6978\n",
      "Recall: 0.6397\n",
      "Precision:0.2181\n",
      "F1 Score: 0.3253\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(g, g.ndata['features'])\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    predictions = torch.sigmoid(logits).squeeze()\n",
    "    \n",
    "    # Apply threshold of 0.5 to classify fraud\n",
    "    predicted_labels = (predictions > 0.5).float()\n",
    "    predicted_labels = predicted_labels[test_mask]  # Apply test_mask here\n",
    "    # Get actual labels\n",
    "    #true_labels = g.edata['isFraud']\n",
    "    true_labels = g.ndata['isFraud'][test_mask]  # Apply test_mask here\n",
    "    # Compute accuracy manually\n",
    "    correct = (predicted_labels == true_labels).sum().item()\n",
    "    total = true_labels.size(0)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # Calculate F1-Score\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision:{precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c244cd-c6db-48cc-b2b6-56fd33b2d3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['features']\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "207ba890-7fc4-4444-a169-f08dfebd5356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(81)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a9353-fc6b-44b8-9eb0-1fe71f7c3004",
   "metadata": {},
   "source": [
    "Accuracy: 0.2478\n",
    "Recall: 0.9758\n",
    "Precision:0.1310\n",
    "F1 Score: 0.2309\n",
    "\n",
    "Accuracy: 0.5616\n",
    "Recall: 0.8884\n",
    "Precision:0.1945\n",
    "F1 Score: 0.3192"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
